import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

class DynamicFocusNet(nn.Module):
    def __init__(self, num_classes=3, pretrained=True):
        super(DynamicFocusNet, self).__init__()
        
        # ==========================================
        # 1. ç‰¹å¾æå–ä¸»å¹² (Backbone) - å€Ÿç”¨ ResNet-18 çš„çœ¼ç›
        # ==========================================
        # æˆ‘ä»¬åŠ è½½é¢„è®­ç»ƒçš„ ResNet-18ï¼Œä½†ä¸è¦å®ƒæœ€åŽçš„æ± åŒ–å±‚å’Œå…¨è¿žæŽ¥å±‚
        resnet = models.resnet18(pretrained=pretrained)
        self.backbone = nn.Sequential(
            resnet.conv1,
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,
            resnet.layer2,
            resnet.layer3,
            resnet.layer4  # è¾“å‡ºç‰¹å¾å›¾å¤§å°ä¸º (Batch, 512, 7, 7)
        )
        
        # ==========================================
        # 2. æ ¸å¿ƒåˆ›æ–°ï¼šç©ºé—´æ³¨æ„åŠ›ç”Ÿæˆæ¨¡å— (Spatial Attention Module)
        # ==========================================
        # å®ƒçš„ä»»åŠ¡æ˜¯çœ‹ç€ 512 é€šé“çš„ç‰¹å¾å›¾ï¼Œæµ“ç¼©å‡ºä¸€å¼  1 é€šé“çš„â€œé»‘ç™½æŽ¢ç…§ç¯â€å›¾
        self.attention_module = nn.Sequential(
            nn.Conv2d(512, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            # æœ€åŽç”¨ 1x1 å·ç§¯åŽ‹ç¼©åˆ° 1 ä¸ªé€šé“ï¼Œå¹¶ç”¨ Sigmoid å°†æ‰€æœ‰å€¼å¡åœ¨ 0~1 ä¹‹é—´
            nn.Conv2d(64, 1, kernel_size=1),
            nn.Sigmoid() 
        )
        
        # ==========================================
        # 3. ç»†ç²’åº¦åˆ†ç±»å¤´ (Fine-grained Classification Head)
        # ==========================================
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(512, num_classes)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šå®šä¹‰å›¾ç‰‡åœ¨ç½‘ç»œä¸­æµåŠ¨çš„æ–¹å‘
        è¾“å…¥ x: (Batch, 3, 224, 224) çš„ X å…‰ç‰‡
        """
        # ç¬¬ä¸€æ­¥ï¼šå…¨å±€æ‰«è§†ï¼Œæå–åŸºç¡€ç‰¹å¾
        # æå–åŽçš„ features ç»´åº¦ä¸º (Batch, 512, 7, 7)
        features = self.backbone(x)
        
        # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆæ³¨æ„åŠ›æƒé‡å›¾ (å¯»æ‰¾ç—…ç¶)
        # attn_mask ç»´åº¦ä¸º (Batch, 1, 7, 7)ï¼Œé‡Œé¢çš„å€¼å…¨åœ¨ 0 åˆ° 1 ä¹‹é—´
        attn_mask = self.attention_module(features)
        
        # ðŸš¨ ç¬¬ä¸‰æ­¥ï¼šå¼ºè¡Œèšç„¦ (åŠ¨æ€æœç´¢çš„æ ¸å¿ƒæ•°å­¦ä½“çŽ°ï¼)
        # æŠŠç‰¹å¾å›¾å’Œæ³¨æ„åŠ›å›¾â€œç›¸ä¹˜â€ã€‚èƒŒæ™¯ï¼ˆæŽ¥è¿‘0ï¼‰è¢«æŠ¹æ€ï¼Œç—…ç¶ï¼ˆæŽ¥è¿‘1ï¼‰è¢«ä¿ç•™
        focused_features = features * attn_mask
        
        # ç¬¬å››æ­¥ï¼šåŸºäºŽçº¯å‡€çš„ç—…ç¶ç‰¹å¾ï¼Œè¿›è¡Œæœ€ç»ˆçš„ç¡®è¯Š
        # èžåˆåŽçš„ç‰¹å¾é€å…¥æ± åŒ–å±‚å’Œå…¨è¿žæŽ¥å±‚
        pooled = self.global_pool(focused_features)
        pooled = torch.flatten(pooled, 1)
        logits = self.classifier(pooled)
        
        # ç¬¬äº”æ­¥ï¼šä¸ºäº†åœ¨è®­ç»ƒæ—¶è®©åŒ»ç”ŸæŽ©ç å½“â€œæ•™éž­â€ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ 7x7 çš„æ³¨æ„åŠ›å›¾æ”¾å¤§å›ž 224x224
        # è¿™æ ·æ‰èƒ½å’Œä½ çš„ ground truth mask è®¡ç®—è¯¯å·®ï¼
        upsampled_attn_mask = F.interpolate(
            attn_mask, 
            size=(224, 224), 
            mode='bilinear', 
            align_corners=False
        )
        
        # è¿”å›žæœ€ç»ˆåˆ†ç±»ç»“æžœ å’Œ æ”¾å¤§åŽçš„æ¨¡åž‹æ³¨æ„åŠ›å›¾
        return logits, upsampled_attn_mask

# ç®€å•æµ‹è¯•ä¸€ä¸‹ç½‘ç»œèƒ½ä¸èƒ½è·‘é€š (æ‰“æ¡©æµ‹è¯•)
if __name__ == '__main__':
    print("æ­£åœ¨å®žä¾‹åŒ–åŠ¨æ€èšç„¦ç½‘ç»œ...")
    model = DynamicFocusNet(num_classes=3, pretrained=False)
    
    # æ¨¡æ‹Ÿè¾“å…¥ä¸€å¼  224x224 çš„ RGB å›¾ç‰‡ (Batch Size = 2)
    dummy_input = torch.randn(2, 3, 224, 224)
    
    print("æ­£åœ¨è¿›è¡Œå‰å‘ä¼ æ’­æµ‹è¯•...")
    out_logits, out_mask = model(dummy_input)
    
    print(f"âœ… æµ‹è¯•æˆåŠŸï¼")
    print(f"åˆ†ç±»è¾“å‡ºç»´åº¦ (Logits): {out_logits.shape}  -> æœŸæœ›æ˜¯ (2, 3)")
    print(f"æ³¨æ„åŠ›å›¾è¾“å‡ºç»´åº¦ (Mask): {out_mask.shape} -> æœŸæœ›æ˜¯ (2, 1, 224, 224)")