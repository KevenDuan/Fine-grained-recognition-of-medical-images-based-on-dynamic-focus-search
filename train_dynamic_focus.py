import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from dataset import COVIDFocusDataset 
from dynamic_focus_net import DynamicFocusNet  # ğŸš¨ å¯¼å…¥æˆ‘ä»¬åˆšåˆšå†™å¥½çš„ç«¯åˆ°ç«¯æ ¸å¿ƒç½‘ç»œ
from tqdm import tqdm
import matplotlib.pyplot as plt

if __name__ == '__main__':
    data_dir = './dataset'

    print("æ­£åœ¨åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
    train_dataset = COVIDFocusDataset(base_dir=data_dir, split='Train', target_size=(224, 224))
    val_dataset = COVIDFocusDataset(base_dir=data_dir, split='Val', target_size=(224, 224))

    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=8)
    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=8)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼å½“å‰è®¡ç®—è®¾å¤‡: {device}")

    # ================= ğŸš¨ æ ¸å¿ƒæ”¹åŠ¨ 1ï¼šä½¿ç”¨å…¨æ–°çš„ç«¯åˆ°ç«¯ç½‘ç»œ =================
    model = DynamicFocusNet(num_classes=3, pretrained=True)
    model = model.to(device)

    # ================= ğŸš¨ æ ¸å¿ƒæ”¹åŠ¨ 2ï¼šå®šä¹‰åŒé‡æŸå¤±å‡½æ•° (è”åˆç›‘ç£) =================
    criterion_cls = nn.CrossEntropyLoss() # ç”¨äºåˆ†ç±»çš„äº¤å‰ç†µæŸå¤±
    criterion_mask = nn.MSELoss()         # ç”¨äºç›‘ç£æ³¨æ„åŠ›å›¾çš„å‡æ–¹è¯¯å·®æŸå¤±
    
    # è¿™æ˜¯ä¸€ä¸ªæå…¶é‡è¦çš„è¶…å‚æ•°ï¼Œå†³å®šäº†â€œæ©ç æ•™é­â€æ‰“å¾—æœ‰å¤šé‡ã€‚
    # 0.5 è¡¨ç¤ºæˆ‘ä»¬æ—¢çœ‹é‡åˆ†ç±»ï¼Œä¹Ÿçœ‹é‡èšç„¦å®šä½ã€‚
    lambda_weight = 0.5 

    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    best_val_acc = 0.0
    
    # è®°å½•æ•°æ®çš„â€œå°æœ¬æœ¬â€ (é¢å¤–å¢åŠ äº†å¯¹ Mask Loss çš„è®°å½•ï¼Œå†™è®ºæ–‡å¿…å¤‡ï¼)
    history_train_loss_total = []
    history_train_loss_cls = []
    history_train_loss_mask = []
    
    history_val_loss_total = []
    history_val_acc = []
    
    num_epochs = 30
    for epoch in range(num_epochs):
        print(f"\n======== Epoch {epoch+1}/{num_epochs} ========")
        
        # ------------------ è®­ç»ƒé˜¶æ®µ ------------------
        model.train()
        running_loss_total = 0.0
        running_loss_cls = 0.0
        running_loss_mask = 0.0
        
        pbar_train = tqdm(train_loader, desc="[è®­ç»ƒé˜¶æ®µ]", unit="batch")
        # æ³¨æ„ï¼šç°åœ¨ dataset åå‡ºä¸‰ä¸ªä¸œè¥¿äº†ï¼images, masks, labels
        for images, masks, labels in pbar_train:
            images, masks, labels = images.to(device), masks.to(device), labels.to(device)
            
            optimizer.zero_grad()
            
            # ================= ğŸš¨ æ ¸å¿ƒæ”¹åŠ¨ 3ï¼šæ¥æ”¶ä¸¤ä¸ªè¾“å‡ºï¼Œè®¡ç®—åŒé‡ Loss =================
            # å‰å‘ä¼ æ’­ï¼šåŒæ—¶å¾—åˆ°åˆ†ç±»ç»“æœ å’Œ é¢„æµ‹çš„æ³¨æ„åŠ›æ©ç 
            outputs, pred_masks = model(images)
            
            # 1. è®¡ç®—åˆ†ç±»ç®—é”™äº†å¤šå°‘
            loss_cls = criterion_cls(outputs, labels)
            # 2. è®¡ç®—æ³¨æ„åŠ›å›¾åç¦»äº†çœŸå®ç—…ç¶å¤šå°‘
            loss_mask = criterion_mask(pred_masks, masks)
            
            # 3. æ€»è¯¯å·® = åˆ†ç±»è¯¯å·® + Î» * æ©ç è¯¯å·® (è¿™è¡Œä»£ç å€¼ä¸€ç¯‡æ ¸å¿ƒè®ºæ–‡ï¼)
            loss_total = loss_cls + lambda_weight * loss_mask
            
            loss_total.backward()
            optimizer.step()
            
            running_loss_total += loss_total.item()
            running_loss_cls += loss_cls.item()
            running_loss_mask += loss_mask.item()
            
            pbar_train.set_postfix({
                'Total': f"{running_loss_total / (pbar_train.n + 1):.3f}",
                'Cls': f"{running_loss_cls / (pbar_train.n + 1):.3f}",
                'Mask': f"{running_loss_mask / (pbar_train.n + 1):.3f}"
            })

        avg_train_loss_total = running_loss_total / len(train_loader)
        history_train_loss_total.append(avg_train_loss_total)
        history_train_loss_cls.append(running_loss_cls / len(train_loader))
        history_train_loss_mask.append(running_loss_mask / len(train_loader))

        # ------------------ éªŒè¯é˜¶æ®µ ------------------
        model.eval() 
        val_loss_total = 0.0
        correct = 0   
        total = 0     
        
        with torch.no_grad():
            pbar_val = tqdm(val_loader, desc="[éªŒè¯é˜¶æ®µ]", unit="batch")
            for images, masks, labels in pbar_val:
                images, masks, labels = images.to(device), masks.to(device), labels.to(device)
                
                outputs, pred_masks = model(images)
                
                loss_cls = criterion_cls(outputs, labels)
                loss_mask = criterion_mask(pred_masks, masks)
                loss_total = loss_cls + lambda_weight * loss_mask
                
                val_loss_total += loss_total.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
        epoch_acc = 100 * correct / total
        avg_val_loss_total = val_loss_total / len(val_loader)
        
        history_val_loss_total.append(avg_val_loss_total)
        history_val_acc.append(epoch_acc)
        
        print(f"ğŸ‘‰ æœ¬è½®æˆç»©å•: Train Loss: {avg_train_loss_total:.4f} | Val Loss: {avg_val_loss_total:.4f} | å‡†ç¡®ç‡: {epoch_acc:.2f}%")
        
        if epoch_acc > best_val_acc:
            best_val_acc = epoch_acc
            torch.save(model.state_dict(), 'dynamic_focus_best_model.pth')
            print(f"ğŸŒŸ æ–°çºªå½•ï¼ç«¯åˆ°ç«¯æ¨¡å‹å·²ä¿å­˜è‡³ dynamic_focus_best_model.pth, å‡†ç¡®ç‡: {best_val_acc:.2f}%")

    # ================= ç»˜åˆ¶æ›´åŠ ä¸°å¯Œçš„è®­ç»ƒæ›²çº¿å›¾ =================
    print("\nè®­ç»ƒç»“æŸï¼Œæ­£åœ¨ç»˜åˆ¶å¤šä»»åŠ¡è”åˆè®­ç»ƒæ›²çº¿å›¾...")
    plt.figure(figsize=(18, 5)) # åŠ å®½ç”»å¸ƒï¼Œç”»ä¸‰å¼ å›¾
    
    # 1. æ€» Loss æ›²çº¿
    plt.subplot(1, 3, 1)
    plt.plot(range(1, num_epochs + 1), history_train_loss_total, label='Train Total Loss')
    plt.plot(range(1, num_epochs + 1), history_val_loss_total, label='Val Total Loss')
    plt.title('Total Joint Loss')
    plt.xlabel('Epochs')
    plt.legend()
    plt.grid(True)
    
    # 2. åˆ†ç±» Loss ä¸ æ©ç  Loss åˆ†è§£å¯¹æ¯” (è®ºæ–‡æ ¸å¿ƒäº®ç‚¹)
    plt.subplot(1, 3, 2)
    plt.plot(range(1, num_epochs + 1), history_train_loss_cls, label='Train Cls Loss', linestyle='--')
    plt.plot(range(1, num_epochs + 1), history_train_loss_mask, label='Train Mask Loss', linestyle='-.')
    plt.title('Loss Breakdown (Cls vs Mask)')
    plt.xlabel('Epochs')
    plt.legend()
    plt.grid(True)
    
    # 3. å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 3, 3)
    plt.plot(range(1, num_epochs + 1), history_val_acc, label='Validation Accuracy', color='green')
    plt.title('Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('joint_training_curve.png', dpi=300)
    print("âœ… å¤šä»»åŠ¡æ›²çº¿å›¾å·²æˆåŠŸä¿å­˜ä¸º joint_training_curve.png")